{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f694a2",
   "metadata": {},
   "source": [
    "# 인공신경망, 오차역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e9dba",
   "metadata": {},
   "source": [
    "- 인간의 지능\n",
    "    - 뇌는 여러 뉴런들이 서로서로 그물처럼 연결되어 있는 신경망(Neural Network)의 구조를 가지고 있다. 바로 뉴런의 작동원리를 알아보자. \n",
    "    - 먼저, 나무의 가지처럼 되어 있는 수상돌기(dendrite)를 통해 자극을 받아들인다. 하지만 이 자극이 너무 작다면, 뉴런에는 아무 일도 일어나지 않는다. 따라서 뉴런에서 신호를 발생시키기 위한 최소한의 자극, 즉 '역치'이상의 자극이 가해져야 한다. <b>만약 충분한 자극으로 전기신호가 발생했다면, 이는 계속해서 전달되어(axon) 나간다.</b> 이 전기신호가 말단부(axon terminal)에 도달하면, 말단부에서 전달물질을 내보내, 다음 뉴런의 수상돌기(dendrite)에 신호를 전달하게 된다. 이렇게 약 1000억개 가량의 뉴런이 그물망처럼 인간의 뇌에 얽혀있는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d130f33",
   "metadata": {},
   "source": [
    "- 인공신경망 \n",
    "    - 뉴런은 우리가 배운 것들과 많이 닮아있다. 예를 들어 로지스틱 회귀(Logistic Regression)를 생각해보자. 데이터 x1, x2, x3(자극)를 받는다. x1, x2, x3에 학습할 가중치 w1, w2, w3를 곱하고 편향 b를 더해 가정을 만들고, 이것을 sigmoid에 넣어 0~1 범위로  만든다. 이 때 가정이 0.5<b> (역치) </b>보다 작다면 '0'으로, 크다면 '1'로 결정한다.\n",
    "    - 뭔가 익숙한 뉴런의 구조는 그렇다 쳐도, 이것을 구체적으로 어떻게 연결해서 무엇을 할 수 있을까? 일단 위 사진을 보자. <b>하나의 노드가 하나의 뉴런이라고 보면 될것이다.</b> 예시 사진은 결과적으로 세개의 데이터를 넣어 하나의 출력을 갖는다. 그 사이에는 값을 주고 받으며 데이터를 계산하는 노드들이 있고, <b>결과적으로 인공지능이 '사고'를 하는 것 처럼 결과를 계산해 낸다. 이 '사고'의 가장 간단한 예로는 XOR문제가 있다.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe35b9",
   "metadata": {},
   "source": [
    "- XOR 문제 \n",
    "    - 우리는 다음과 같이 0, 1로 이루어져 있는 데이터를, Wx+b의 가정 직선에서 W, b를 학습시켜 직선으로 찾아가는 과정으로 알고있다. 우리가 선형(Wx+b)인 가정을 한다는 것은 하나의 직선을 긋는 것과 비슷하다. 그리고 우리가 만든 뉴런, 즉 노드 역시 각각이 하나의 직선을 긋는 것과 비슷하다. \n",
    "    - XOR은 두개의 값이 다를 때만, True(=1)을 반환하는 논리함수이다. \n",
    "    - 우리는 정답 W와 b를 머신러닝으로, 즉 학습을 통해서 알아내는 방법을 찾아야 한다. 예를 들어 학습할 데이터 (x1, x2, Y)가 있다고 하자. x1, x2를 입력하고 노드(뉴런)를 거쳐 나온 Y'와 실제 Y를 비교해 이 오차가 줄어들도록 각 노드의 W, b를 학습시켜야 할 것이다. 사실 신경망에서 각 노드의 가중치와 편향을 학습시키는 방법은 인간이 몇 년동안이나 헤맸던 문제이다. 몇 번의 시행착오는 있었지만 결국 오차역전법이라는 방법을 찾아낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a6fa4",
   "metadata": {},
   "source": [
    "- 오차역전파\n",
    "    - 학습의 목적은 결국 비용을 줄이는 것이다. 비용은 결국 OUTPUT으로 나온 마지막 결과와 실제 Y의 차이가 되고, 우리는 이것을 경사하강법을 통해서 비용의 최솟값을 구했었다. 문제는 우리가 이 전에 정의했던 비용함수 cost(W)처럼 간단하게 W 하나로 OUTPUT이 결정된 게 아니라는 것이다. <b>각 노드가 가진 각각의 W, b가 모두 OUTPUT이 나오는데 관여를 했기 때문이다. 따라서 간단하게 cost(W)함수를 그리고 경사를 따라 하강하는 것은 불가능하다는 것을 안다.</b>\n",
    "    - 따라서 우리는 노드 각각을 경사하강시켜줄 것이다. 어떤 노드가 OUTPUT을 내는데에 얼마나 관여를 했는지를 편미분으로 계산하고, 이 기울기를 따라 각각의 W, b를 학습시켜줄 것이다. 편미분은 고등학교 때 배운 합성함수의 미분을 생각하면 쉽다.\n",
    "    - 우리가 <b>f(g(x))를 x에 대해 미분한 것을 보는 행위는 x가 얼마나 f(g(x))에 관여하고 있는가(기울기)를 보는 것과 같다.</b> 결과적으로 식을 보면 먼저 f를 g에 대해 미분하여 f에 대한 g가 얼마나 관여하고 있는가 보고, 거기에 g를 x로 미분한 것을 곱해서 우리가 원하는 f(g(x))에 대한 x의 미분값을 얻게된다.\n",
    "    - 신경망도 이와 비슷하다. OUTPUT노드에 어떤 INPUT노드가 얼마나 관여했나 확인하기 위해서는 자신의 앞 노드에 대한 자신의 미분값을 계속해서 재귀적으로 곱해나가다 보면 알 수 있다. 합성함수에서 x는 자신의 앞에 있는 g를 , g는 자신의 앞에 있는 f를 미분해서 곱한 것처럼. \n",
    "    - 결과적으로 각각 비용에 대한 자신의 기여도를 계산하여 경사를 하강할 수 있게 되고 이렇게 경사를 하강하며 학습을 진행할 수 있게 된다. 이 모양이 마치 오차가 거꾸로 전파되는 것 같다고 해서 오차역전파(Back-Propagation)라고 한다. 텐서플로우 역시 모든 것이 텐서로 이루어진 그래프(노드, 엣지)로 되어있기 때문에, 오차가 그래프를 타고 내려오면서 변수를 학습시킨다고 생각할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbfd6a",
   "metadata": {},
   "source": [
    "- XOR 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "602d412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a24b22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b54f1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88b297",
   "metadata": {},
   "source": [
    "    - 먼저 학습할 정답 데이터 x_data, y_data 쌍이 주어진다. 우리의 목표는 이 주어진 데이터를 바탕으로 신경망을 학습시켜, 새로운 임의의 x를 입력했을 때 그 결과 y가 '0'인지 '1'인지 구별할 수 있도록 인공지능을 만드는 것이다. 위의 데이터를 로지스틱 회귀에서 썼던 코드에 그대로 넣어보자. 위에서 보았듯이 하나의 직선으로는 XOR을 구분할 수 없음을 알 수 있다. 정확도가 0.5, 즉 제대로 학습이 되지 않은 채 0과 1을 절반의 확률로 찍는것과 다름이 없는 결과가 나옴을 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "277bb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random.normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66cccb",
   "metadata": {},
   "source": [
    "    - 그래서 신경망을 추가했다. 사실 2개의 레이어면 충분하지만, 넓게 신경망을 펼쳐도 전혀 상관없다. 각각의 노드들이 그 상황에 맞게 오차를 최소화하는 방향으로 각자 경사를 타고 하강할 것이고, 각자 최적의 가중치와 편향을 찾을 것이다.\n",
    "\n",
    "    - 위의 코드는 노드가 4개 있는 것이 아니다. 각 노드들을 레이어별로 하나의 행렬로 합쳐놓은 것이고, 4개의 레이어가 있는 것이다. 그리고 W1, W2, ... 의 행렬은 [입력, 출력] 의 shape을 가지게 된다. 행렬 곱은 (n × m) (m × k)의 꼴로 앞 행렬의 열 크기와, 뒷 행렬의 행 크기가 같아야 연산할 수 있다. 따라서 신경망의 각 레이어도 앞 레이어의 출력 갯수와 뒷 레이어의 입력 갯수가 같도록 설정해야 한다. 단, 우리는 2개의 데이터를 넣어 1개의 결과(0 또는 1)를 얻을 것이기 때문에, 가장 첫 레이어는 [2, ?]의 꼴, 가장 뒷 레이어는 [?, 1]의 꼴을 가져야만 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31377a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3415239\n",
      "100 0.6931134\n",
      "200 0.6887698\n",
      "300 0.6841522\n",
      "400 0.6788374\n",
      "500 0.6723188\n",
      "600 0.6640307\n",
      "700 0.65352225\n",
      "800 0.6405759\n",
      "900 0.6249241\n",
      "1000 0.60609984\n",
      "1100 0.58385396\n",
      "1200 0.5585429\n",
      "1300 0.531108\n",
      "1400 0.5026169\n",
      "1500 0.4733172\n",
      "1600 0.44054282\n",
      "1700 0.39287004\n",
      "1800 0.31726885\n",
      "1900 0.2282205\n",
      "2000 0.15467384\n",
      "2100 0.10753919\n",
      "2200 0.07817239\n",
      "2300 0.05922523\n",
      "2400 0.046494573\n",
      "2500 0.037602123\n",
      "2600 0.031168025\n",
      "2700 0.026366182\n",
      "2800 0.022684917\n",
      "2900 0.019796459\n",
      "3000 0.017484233\n",
      "3100 0.015600844\n",
      "3200 0.014043358\n",
      "3300 0.01273823\n",
      "3400 0.011631754\n",
      "3500 0.01068398\n",
      "3600 0.0098646255\n",
      "3700 0.009150428\n",
      "3800 0.0085232835\n",
      "3900 0.00796888\n",
      "4000 0.007475815\n",
      "4100 0.0070349174\n",
      "4200 0.006638561\n",
      "4300 0.006280729\n",
      "4400 0.0059562325\n",
      "4500 0.005660761\n",
      "4600 0.0053908536\n",
      "4700 0.005143296\n",
      "4800 0.0049157175\n",
      "4900 0.004705691\n",
      "5000 0.0045115417\n",
      "5100 0.004331449\n",
      "5200 0.004164056\n",
      "5300 0.004008159\n",
      "5400 0.0038625696\n",
      "5500 0.0037264146\n",
      "5600 0.003598733\n",
      "5700 0.0034788777\n",
      "5800 0.0033662026\n",
      "5900 0.0032599717\n",
      "6000 0.0031597796\n",
      "6100 0.0030651158\n",
      "6200 0.002975515\n",
      "6300 0.0028906916\n",
      "6400 0.0028101662\n",
      "6500 0.0027336983\n",
      "6600 0.002661018\n",
      "6700 0.0025917953\n",
      "6800 0.0025258209\n",
      "6900 0.002462884\n",
      "7000 0.0024027755\n",
      "7100 0.002345375\n",
      "7200 0.002290458\n",
      "7300 0.0022378748\n",
      "7400 0.0021874905\n",
      "7500 0.0021391697\n",
      "7600 0.0020927188\n",
      "7700 0.0020482268\n",
      "7800 0.0020054542\n",
      "7900 0.001964222\n",
      "8000 0.0019246341\n",
      "8100 0.0018864516\n",
      "8200 0.0018496595\n",
      "8300 0.0018142124\n",
      "8400 0.0017800211\n",
      "8500 0.0017469508\n",
      "8600 0.0017150609\n",
      "8700 0.0016842323\n",
      "8800 0.0016543899\n",
      "8900 0.0016255339\n",
      "9000 0.001597649\n",
      "9100 0.0015705411\n",
      "9200 0.0015444043\n",
      "9300 0.0015190295\n",
      "9400 0.0014943569\n",
      "9500 0.0014705057\n",
      "9600 0.001447342\n",
      "9700 0.0014248055\n",
      "9800 0.001402986\n",
      "9900 0.0013817042\n",
      "10000 0.0013611095\n",
      "\n",
      "Hypothesis:  [[9.4062090e-04]\n",
      " [9.9893308e-01]\n",
      " [9.9863303e-01]\n",
      " [2.0651221e-03]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d9661",
   "metadata": {},
   "source": [
    "    - 앞서 이야기한 오차역전법도 텐서플로우의 함수로 풀어볼 수 있다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
