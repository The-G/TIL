{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c915ba4b",
   "metadata": {},
   "source": [
    "# 강화학습 알고리즘의 종류 (분류)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed838430",
   "metadata": {},
   "source": [
    "출처 : https://dacon.io/forum/406104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531da92b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0998a",
   "metadata": {},
   "source": [
    "## 1.강화학습이란?\n",
    "### 주어진 환경과 상호작용하여 좋은 점수를 얻는 방향으로 성장하는 머신러닝 분야를 '강화학습(reinforcement learning)'이라고 합니다.\n",
    "- 게임(예: 아타리, 마리오)에서 주로 사용되었으며 인간과 동등하거나 그 이상의 성능을 보였습니다. <br>\n",
    "- 최근에는 알고리즘이 신경망의 조합과 함께 발전함에 따라 진자 문제와 같은 보다 복잡한 작업을 해결할 수 있게 되었습니다. <br>\n",
    "- 강화학습은 크게 <b>상태(state), 에이전트(agent), 행동(action), 보상(reward)</b> 4가지 요소로 나눌 수 있습니다. <br>\n",
    "- 에이전트(agent) : 인공지능 플레이어입니다. <br>\n",
    "- 환경(state) : 에이전트가 솔루션을 찾기 위한 무대입니다. <br>\n",
    "- 행동(action) : 에이전트가 환경 안에서 시행하는 상호작용입니다. <br>\n",
    "- 보상(reward) : 에이전트의 행동에 따른 점수 혹은 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189685c",
   "metadata": {},
   "source": [
    "## 2. 에이전트(agent)의 구성요소?\n",
    "강화학습의 분류 체계를 알아보기 전에 먼저 분류의 기준이 되는 강화학습 agent(행위자)의 구성 요소에 대해 알아보아야 합니다. 강화학습의 agent는 크게 다음 세가지의 요소를 갖습니다.\n",
    "\n",
    "### 1. Policy\n",
    "Agent의 행동 패턴입니다. 주어진 환경(state)에서 어떤 행동(action)을 취할지 말해줍니다. 즉, 환경(state)을 행동(action)에 연결 짓는 함수입니다.\n",
    "Policy는 크게 deterministic(결정적) policy와 stochastic(확률적) policy로 나뉩니다. \n",
    "Deterministic policy는 주어진 환경(state)에 대해 하나의 행동(action)을 주고, stochastic policy는 주어진 환경(state)에 대해 행동(action)들의 확률 분포를 줍니다.\n",
    "\n",
    "\n",
    "### 2. Value function\n",
    "환경(State)과 행동(action)이 나중에 어느 정도의 보상(reward)을 돌려줄지에 대한 예측 함수입니다. \n",
    "즉, 해당 환경(state)과 행동(action)을 취했을 때 이후에 받을 모든 보상(reward)들의 가중합입니다. \n",
    "이때, 뒤에 받을 보상(reward) 보다 먼저 받을 보상(reward)에 대한 선호를 나타내기 위해 discounting factor λ를 사용합니다.\n",
    "\n",
    "\n",
    "### 3. Model\n",
    "다음 환경(state)과 보상(reward)이 어떨지에 대한 agent의 예상입니다. State model과 Reward model로 나눌 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1ec211",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (81478367.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/1t/6yr_b7vx0bn5412mzpsfksyc0000gn/T/ipykernel_7549/81478367.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    애매하네...\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "애매하네..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f0129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
