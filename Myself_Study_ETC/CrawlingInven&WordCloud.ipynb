{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo thing\n",
    "1. crawling using selenium\n",
    "2. preprocessing data\n",
    "    - synonym\n",
    "    - dictionary about game\n",
    "3. set WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference : https://beomi.github.io/2017/02/27/HowToMakeWebCrawler-With-Selenium/ <br>\n",
    "감사합니다 준범님!! ㅎㅎ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium이란?\n",
    "- Selenium은 주로 웹앱을 테스트하는데 이용하는 프레임워크다. webdriver라는 API를 통해 운영체제에 설치된 Chrome등의 브라우저를 제어하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selenium 설치\n",
    "- pip install selenium\n",
    "\n",
    "#### BeautifulSoup 설치\n",
    "- pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webdriver\n",
    "#### ChromeWebDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhantomJS webdriver\n",
    "- PhantomJS는 기본적으로 WebTesting을 위해 나온 Headless Browser다.(즉, 화면이 존재하지 않는다.)\n",
    "- 하지만 JS등의 처리를 온전하게 해주며 CLI환경에서도 사용이 가능하기 때문에, 만약 CLI서버 환경에서 돌아가는 크롤러라면 PhantomJS를 사용하는 것도 방법이다. 오!!\n",
    "- PhantomJS는 http://phantomjs.org/download.html 에서 다운로드!!\n",
    "- 압축을 풀어주고 bin/phantomjs라는 파일을 사용하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium으로 사이트 브라우징 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\phantomjs\\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead\n",
      "  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '\n"
     ]
    }
   ],
   "source": [
    "# Chromedriver의 위치를 지정\n",
    "# driver = webdriver.Chrome('D:/To_Crawling/chromedriver')\n",
    "# PhantomJS의 위치를 지정(R 에서도 이렇게 가능한가?!)\n",
    "driver = webdriver.PhantomJS('D:/To_Crawling/phantomjs-2.1.1-windows/bin/phantomjs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selenium은 기본적으로 웹 자원들이 모두 로드될 때 까지 기다려주지만, 암묵적으로 모든 자원이 로드 될 때 까지 <br> 기다리게하는 시간을 직접 implicitly_wait를 통해 지정할 수 있다.\n",
    "2. 3초 후 Load\n",
    "3. url 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "driver = webdriver.Chrome('D:/To_Crawling/chromedriver')\n",
    "# 2\n",
    "driver.implicitly_wait(3)\n",
    "# 3\n",
    "driver.get('https://naver.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium은 driver 객체를 통해 여러가지 메소드를 제공한다.\n",
    "- URL에 접근하는 API\n",
    "    - get('http://url.com')\n",
    "- 페이지의 단일 element에 접근하는 API\n",
    "    - find_element_by_name('HTML_name')\n",
    "    - find_element_by_id('HTML_id')\n",
    "    - find_element_by_xpath('/html/body/some/xpath')\n",
    "- 페이지의 여러 elements에 접근하는 API\n",
    "    - find_element_by_css_selector('#css > div.selector') # '#'은 id / '.'은 class 나타냄 \n",
    "    - find_element_by_class_name('SomeClassName')\n",
    "    - find_element_by_tag_name('h1')\n",
    "    \n",
    "위 메소드를 활용하면 html 브라우저에서 파싱해주기 때문에 굳이 Python과 BeautifulSoup을 사용하지 않아도 된다. (R에서 그렇게 함) <br>\n",
    "하지만 Selenium에 내장된 함수만 사용가능하기 때문에 좀 더 사용이 편리한 soup객체를 이용하려면 driver.page_source API를 <br> 이용해 현재 랜더링 된 페이지의 Elements를 모두 가져올 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naver login 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네이버는 requests를 이용해 로그인하는 것이 어렵다. 프론트 단에서 JS처리를 통해 로그인 처리를 하기 때문인데, Selenium을 이용하면 아주 쉽게 로그인을 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome('D:/To_Crawling/chromedriver')\n",
    "driver.get('https://nid.naver.com/nidlogin.login')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네이버 로그인 화면을 확인해 보면 아이디를 입력받는 부분의 name이 id, 비밀번호를 입력받는 부분의 name이 pw인 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('D:/To_Crawling/chromedriver')\n",
    "driver.get('https://nid.naver.com/nidlogin.login')\n",
    "\n",
    "# 아이디 / 비밀번호를 입력해 준다. \n",
    "naver_id = ''\n",
    "naver_pw = ''\n",
    "driver.find_element_by_name('id').send_keys(naver_id)\n",
    "driver.find_element_by_name('pw').send_keys(naver_pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성공적으로 값이 입력된 것을 확인 할 수 있다. <br>\n",
    "이제 login버튼을 눌러 실제로 로그인이 되는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 로그인 버튼을 눌러주자.\n",
    "driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/input').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아이디 저장 부분 넘기기 \"저장안함\" 으로\n",
    "driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/span[2]/a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로그인이 필요한 네이버 페이의 주문내역 페이지를 가져와 보자!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naver 페이 들어가기\n",
    "driver.get('https://order.pay.naver.com/home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html # 전체를 다 긁어옴!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser') # 보기좋게 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "notices = soup.select ('div.p_inr > div.p_info > a > span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for each_text in notices:\n",
    "    print(each_text.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로그인이 잘 되고, 성공적으로 리스트를 받아오는 것을 확인해 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리하기\n",
    "- Selenium은 웹 테스트 자동화 도구이지만, 멋진 크롤링 도구로 사용할 수 있다. \n",
    "- 또한 BeautifulSoup과 함께 사용도 가능하기 때문에 크롤링을 하는데 제약도 줄어 훨신 쉽게 크롤링을 할 수 있다. \n",
    "\n",
    "- 다음 가이드 나만의 웹 크롤러 만들기(4): Django로 크롤링한 데이터 저장하기 도 정리하자!!\n",
    "- https://beomi.github.io/2017/03/01/HowToMakeWebCrawler-Save-with-Django/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
